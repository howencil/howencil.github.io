---
title: "如何保证消息不重复消费，如何保证消息不丢失，如何实现消息消费顺序性"
date: 2022-06-07T16:18:14+08:00
draft: false
author: 插画师
tags: ["消息中间件MQ"]
categories: ["Tech"]
---

## 1.如何保证消息不重复消费
#### 1.出现重复消费的场景
1. 首先各类消息中间件都有可能出现消息重复消费的问题，因为此类问题通常不是由MQ来保证的，而是消费方自己保证。
2. 举例kafka来说明重复消费问题，kafka有一个offset的概念，每个消息写进去，都会有一个offset代表此条消息的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交，代表已经消费过了，下次就是重启，kafka就会让消费者从上次消费到的offset来继续消费。*但凡事有例外*，如果consumer已经消费了数据，但是还没来得及提交offset就已经挂了，那么重启之后就会收到重复数据。
![](/如何保证消息不重复消费，如何保证消息不丢失，如何实现消息消费顺序性/1.png)

#### 2.保证幂等性
1. 可在内存中维护一个set，只要从消息队列中获取到一个消息，先查询这个消息在不在set里，如果在表示已消费过，直接丢弃；如果不在，则在消费后将其放入set中。
2. 如果要写数据库，可以拿唯一键先去数据库查询，如果不存在则写入，如果存在则更新或者直接丢弃。
3. 如果是写Redis那就没问题，每次都是set，具有天然的幂等性。
4. 让生产者发送消息时，每条消息添加一个全局的唯一ID，然后消费时，将该ID保存到Redis里。消费时先去Redis里查询，根据情况进行消费或丢弃。
5. 数据库操作可以设置唯一键，防止重复数据插入，这样插入只会报错而不会插入重复数据。

## 2.如何保证消息不丢失
> [关于MQ的几件小事（四）如何保证消息不丢失 - 掘金](https://juejin.cn/post/6844903849099018253)

#### 1.丢失数据的场景
###### 1.RabbitMQ
1. **生产者丢失数据：** 生产者在发送数据到mq时，传输过程中可能会因为网络等问题而将数据弄丢。
	1.  **1.RabbitMQ开启事务功能：** 在生产者发送数据之前开启事务，然后发送消息，如果消息没有成功被RabbitMQ接收到，那么生产者就会收到异常报错，这时候就可以回滚事务，然后尝试重新发送，如果收到了消息，则提交事务。 2. **缺点：** RabbitMQ事务已开启，就会变成同步阻塞操作，生产者会阻塞等待是否发送成功，太消耗性能会造成吞吐量下降。
	2.  **1.开启confirm模式（最常用的方案）：** 在生产者哪里设置开启了confirm模式之后，每次写的消息都会分配一个唯一的ID，然后写入RabbitMQ后，RabbitMQ会回传一个ack消息，代表消息发送成功；如果RabbitMQ没能处理这个消息，会回调一个nack接口，代表消息发送失败，可以进行重试。结合这个机制可以自己在内存里维护每个消息的ID，如果超过一定时间还没接收到这个消息的回调，那么可以进行重发。
	3.  **二者不同：** 事务机制是同步的，提交一个事务之后会造成阻塞，但confirm是异步的，发送消息之后可以接着发送下一个消息，然后RabbitMQ会回调告知成功与否。*一般在生产者这块避免丢失，都是用confirm机制*。
2. **RabbitMQ自己丢失数据：** 1.如果没有RabbitMQ的持久化，那么RabbitMQ一旦重启，那么数据就丢失了；2.RabbitMQ开启了持久化，但是还没来得及持久化自己就挂了，这样可能导致一部分数据丢失。
	1. **设置持久化：** 设置持久化有2个步骤：1.创建queue时将其设置为持久化，这样就可以保证RabbitMQ持久化queue的元数据，但不会持久化queue里的数据；2.发送消息时将消息的deliveryMode设置为2，这样消息就会被设置为持久化方式，此时RabbitMQ就会将消息持久化到磁盘上。*必须要同时开启这2个才可以。*
	2. 持久化可以合生产者的confirm机制结合起来，只有持久化到磁盘后，才会通知生产者ack，这样就算在持久化之前RabbitMQ挂了，数据丢了，生产者没有收到ack回调也会进行消息重发。
3. **消费者弄丢数据：** 消费者在消费时，刚消费到还没处理，消费者自己挂了，重启之后，RabbitMQ认为你已经消费过了，然后就丢了数据。
	1. **消费者弄丢了数据：** 使用RabbitMQ提供的ack机制，首先关闭RabbitMQ的自动ack，然后每次在确保处理完这个消息后手动调用ack，这样可以避免消息还没有处理完就ack。

###### 2.kafka
1. **消费者弄丢数据：** 消费者消费到消息后，自动提交了offset，但是在处理消息时自己挂掉了，那么这条消息就丢失了。
	1. **解决方案：** 关闭自动提交offset，在自己处理完毕之后手动提交offset，这样就不会丢失数据。
2. **kafka弄丢数据：** 比如kafka的某个broker宕机了，然后重新选举partition的leader时。如果此时follow还没来得及同步数据，leader就挂了，然后某个follow成为了leader，它就少了一部分数据。
3. **生产者弄丢数据：** 生产者没有设置相应的策略，发送过程中丢失数据。
